{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLakeQTable.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmEktNnSRTuY",
        "colab_type": "text"
      },
      "source": [
        "# Grid World - Frozen Lake\n",
        "\n",
        "In this example we will look at [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0) a simple grid world example from [OpenAI Gym](https://gym.openai.com/). \n",
        "\n",
        "In this world, an agent has to navigate between a starting and an endpoint across a grid of possible positions.\n",
        "In the setting of a frozen lake the agent has to retrieve an object (a frisbee) that has been thrown on the the ice covering the lake.\n",
        "\n",
        "Each grid position is one of:\n",
        "   * S: the starting point\n",
        "   * F: frozen surface, safe to cross\n",
        "   * H: hole, fall into the lake, the episode ends unsuccessfully\n",
        "   * G: goal, object is retrieved and the reward obtained.\n",
        "\n",
        "If the goal G is reached, the episode ends and the agent receives a reward of 1, if the agent falls into a hole, the episode ends and the reward is set to 0.\n",
        "\n",
        "The grid-world can be explored in two settings: deterministic or stochastic.\n",
        "In the deterministic setting, the action of the agent, i.e. in which direction\n",
        "they travel, is always the intended direction.\n",
        "In the stochastic setting, the agent goes into the intended direction with a probability of 1/3, i.e. 0.3333, and into one of two other directions with probability of 1/3.\n",
        "This setting is controlled with the parameter ```is_slippery``` when setting up the environment.\n",
        "\n",
        "Additionally, a larger environment with a 8x8 grid instead of the 4x4 grid used here exists.\n",
        "\n",
        "We will use the deterministic setting in this exercise but you should explore the stochastic one as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEkQ9pVyTVtM",
        "colab_type": "code",
        "outputId": "00dd4aa7-0285-4560-f2c7-e5c1b3b9e6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# required imports\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "# large figures\n",
        "rcParams['figure.figsize'] = 8, 6\n",
        "\n",
        "# clear the output of the cell to avoid long output lists\n",
        "# see http://ipython.org/ipython-doc/dev/api/generated/IPython.display.html#IPython.display.clear_output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# for the animation at the end\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['random']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buw4yFWBQ9GO",
        "colab_type": "code",
        "outputId": "18d1cbbc-7b92-48c7-9889-44150e382b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# not sure we need it - but check if we're running in Google's free Colaboratory.\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print (IN_COLAB)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxnSvQZiTOnn",
        "colab_type": "text"
      },
      "source": [
        "## Explore the environment\n",
        "\n",
        "We now load the game engine from OpenAI Gym and look at an example of the frozen lake grid world the agent has to master.\n",
        "\n",
        "The agent always starts in the top left corner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE969o2JTtet",
        "colab_type": "code",
        "outputId": "0f1d8b0c-50aa-4d98-9b5c-6bb7babb7517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# smaller grid with a 4x4 grid  : FrozenLake-v0\n",
        "# larger version with a 8x8 grid: FrozenLake8x8-v0\n",
        "#env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8LE7PeZT8ec",
        "colab_type": "text"
      },
      "source": [
        "The agent can follow one of 4 actions that are available\n",
        "from the ```action_space```. In our case, we can move up, down, left, right.\n",
        "Hence, there are four discrete actions, the specific meaning is encoded\n",
        "in the environment.\n",
        "\n",
        "The environment is based on an nxn grid and hence has $n^2$ states. For the 4x4 Frozen Lake example, this means we have 16 states, one for each position the agent can be on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrFnofZxT8pE",
        "colab_type": "code",
        "outputId": "836e9f53-eb0b-41d6-f206-09e00e2a9247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# there are two ways to obtain the number of actions:\n",
        "# env.nA and env.action_space.n\n",
        "print('Number of actions: {}'.format(env.nA))\n",
        "print('Action space: {}'.format(env.action_space))\n",
        "\n",
        "# there are two ways to obtain the numbers of states: \n",
        "# env.nS and env.observation_space.n\n",
        "print('Number of states: {}'.format(env.nS) )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of actions: 4\n",
            "Action space: Discrete(4)\n",
            "Number of states: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9RuYbGsVcY4",
        "colab_type": "text"
      },
      "source": [
        "We can try how an agent moves through the grid world if we randomly choose an action - we cannot expect to reach the goal G but we can observe the movement of the agent.\n",
        "\n",
        "We use the method ```step(action)``` to execute an action the agent chooses.\n",
        "This method returns the following values:\n",
        "   * bservation: The next  state of the environment after executing the action, in our case the positon on the grid world\n",
        "   * reward, in our case 0 or 1\n",
        "   * done: indicates that the episode has ended\n",
        "   * info: dictionary with debugging information - agents are not allowed to use this information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3yIa8VoVpuc",
        "colab_type": "code",
        "outputId": "c80be52a-a757-4871-abda-91dd3a529a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "observation = env.reset()\n",
        "for i in range(0,5):\n",
        "  print('Step {}'.format(i))\n",
        "  #visual output\n",
        "  env.render()\n",
        "  print('Observation: {}'.format(observation))\n",
        "\n",
        "  #get a random action\n",
        "  a = env.action_space.sample()\n",
        "  print('Action: {}'.format(a))\n",
        "\n",
        "  #execute the action\n",
        "  observation, reward, done, info =  env.step(a)\n",
        "  print('Reward {}, Info {}'.format(reward, info))\n",
        "\n",
        "  if done:\n",
        "    print('End of episode')\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Observation: 0\n",
            "Action: 1\n",
            "Reward 0.0, Info {'prob': 1.0}\n",
            "Step 1\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Observation: 4\n",
            "Action: 3\n",
            "Reward 0.0, Info {'prob': 1.0}\n",
            "Step 2\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Observation: 0\n",
            "Action: 3\n",
            "Reward 0.0, Info {'prob': 1.0}\n",
            "Step 3\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Observation: 0\n",
            "Action: 3\n",
            "Reward 0.0, Info {'prob': 1.0}\n",
            "Step 4\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Observation: 0\n",
            "Action: 2\n",
            "Reward 0.0, Info {'prob': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NXD7bfKbepF",
        "colab_type": "text"
      },
      "source": [
        "We can manually set the agent to be in a specific state. In case of the Frozen Lake grid world, setting the state means we move the agent to a specific location on the grid.\n",
        "Starting from 0, the position ```s=2``` is the third position in the first row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W6dmRijbfTI",
        "colab_type": "code",
        "outputId": "b6ee15f6-5a3e-476b-84a5-0a094bd31f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "env.env.s=2\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5EI1C5Rcz7m",
        "colab_type": "text"
      },
      "source": [
        "The function ```env.P```  represents the transition probabilities and can be used to evaluate the states and rewards for a specific state.\n",
        "It is organized as ```env.P[state][action]``` and contains a list of the variables: ```probability```, ```next state```, ```reward```, ```done```.\n",
        "\n",
        "In the deterministic setting, each transition probability has only one possible next state for each action, in the stochastic setting, each transition can result in one of three possible new states, the probability to end up in any of them is 1/3.\n",
        "\n",
        "For example, for state env.s=2, we would have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAOSGkqbcM4",
        "colab_type": "code",
        "outputId": "51329569-8312-47b5-b562-e45d70b34ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "s=2\n",
        "env.env.s=s\n",
        "env.render()\n",
        "for a in range(0, env.nA):\n",
        "  print('P for State {}, action {}: {}'.format(s,a,env.P[s][a]) )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "P for State 2, action 0: [(1.0, 1, 0.0, False)]\n",
            "P for State 2, action 1: [(1.0, 6, 0.0, False)]\n",
            "P for State 2, action 2: [(1.0, 3, 0.0, False)]\n",
            "P for State 2, action 3: [(1.0, 2, 0.0, False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRWRyiBCypz2",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning\n",
        "\n",
        "We now use Q-learning to train the agent.\n",
        "In Q learning, we use the best action for a current state (exploitation), apart from a small exploratory part determined by a small probability $\\epsilon$ were we choose a random action to learn how the environment reacts to our actions.\n",
        "\n",
        "The Q-learning update rule is:\n",
        "$$ Q(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha (r + \\gamma \\,\\mathrm{max}_a Q(s^\\prime,a))$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, $\\gamma$ the discount factor, $s$ is the current state, $s^\\prime$ is the next state and $a$ are all actions.\n",
        "\n",
        "In this implementation we use a tabular approach, i.e. we record all Q-values in a big table.\n",
        "\n",
        "During training, we learn from the environment by executing actions and observe new states and rewards until an episode ends. Then we repeat the process for a large number of episodes to fill the Q-table accurately.\n",
        "\n",
        "Since the environment is stochastic and we can fail easily, the\n",
        "exploration probability needs to be fairly large, though \n",
        "we could decrease it as the training progresses and the Q-value\n",
        "table is filled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR_WJMxmgK5K",
        "colab_type": "code",
        "outputId": "d1c52bf2-439d-455f-d57b-57853ae80515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# greedy exploration probability\n",
        "eps = 0.3\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.3\n",
        "\n",
        "#discount factor\n",
        "gamma = 0.95\n",
        "\n",
        "#number of episodes to train over:\n",
        "n_episodes = 5000\n",
        "\n",
        "# table of Q values\n",
        "q_table = np.zeros([env.nS, env.nA])\n",
        "\n",
        "# counter for how long each epoch lasted\n",
        "epoch_list = []\n",
        "reward_list = []\n",
        "\n",
        "# loop over all episodes \n",
        "# (let the index run from 1 instead of 0 as we would count from 1)\n",
        "for i in range(1, n_episodes+1):\n",
        "  if i% 10 == 0:\n",
        "    clear_output(wait=True)\n",
        "    print('Now in episode {}'.format(i))\n",
        "\n",
        "  # initialize each new episode\n",
        "  state = env.reset()  # set the environment back to starting setting\n",
        "                       # this returns the first observation, i.e. the \n",
        "                       # starting state environment is in.\n",
        "  epochs  = 0          # counter how long a given episode runs until it terminates\n",
        "  rSum    = 0          # overall reward for this episode\n",
        "  done    = False\n",
        "  j       = 0\n",
        "\n",
        "  # now interact with the environment until the episode ends\n",
        "  # i.e. we either reach the goal or fall into a hole\n",
        "  # protect against run-away processes and limit to 500 steps per episode\n",
        "  while not done and j < 500:\n",
        "\n",
        "    j += 1\n",
        "\n",
        "    # choose an action - random action with probability epsilon \n",
        "    #  exploration, best currently known action with probability (1-epsilon)\n",
        "    randNo = random.uniform(0,1)\n",
        "    if randNo < eps:\n",
        "      action = env.action_space.sample() # choose random action\n",
        "    else:\n",
        "      action = np.argmax(q_table[state]) # choose current best action\n",
        "\n",
        "    \n",
        "\n",
        "    # now interact with the environment: execute the action and observe new state\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    #if reward >0:\n",
        "    #  print ('Reward: {}'.format(reward))\n",
        "\n",
        "    # add a small negative penalty if we're not successful, \n",
        "    # helps to speed up training (at least in deterministic case)\n",
        "    if reward==0 :\n",
        "      reward=-0.001\n",
        "\n",
        "    # update Q-values\n",
        "    Q_now     = q_table[state, action]       # what we currently know from the table\n",
        "    max_value = np.max(q_table[next_state])  # new best Q value\n",
        "\n",
        "    q_table[state, action] = (1-alpha)*Q_now + alpha*(reward + gamma*max_value)\n",
        "\n",
        "\n",
        "    # increase counter\n",
        "    epochs += 1\n",
        "\n",
        "    # save total reward\n",
        "    rSum   += reward\n",
        "\n",
        "    # end of interaction, the new state becomes the current state\n",
        "    state = next_state\n",
        "  else:\n",
        "    epoch_list.append(epochs)\n",
        "    reward_list.append(rSum)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now in episode 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkWuthLlJPI6",
        "colab_type": "code",
        "outputId": "04ffcb9e-bd89-4b7c-faad-7ff8dff20dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Average number of steps in each episode\n",
        "print(int(np.mean(epoch_list)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWvKtLwEnjmr",
        "colab_type": "text"
      },
      "source": [
        "How a distribution of how long each episode was, i.e. how long the agent needed\n",
        "to reach the goal or fall into a hole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ovlanUqnkbD",
        "colab_type": "code",
        "outputId": "54bae816-cc06-4559-d4f0-c42903a3eaef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.hist(epoch_list)\n",
        "plt.title('Episode Length')\n",
        "plt.xlabel('Episode length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAZ0UlEQVR4nO3de7glVX3m8e8raEQwAQQZbGgataOSUZG0SoIZSYgIokEzinhFR21NcNSJRlt0Auo4QzIJKokSUZmAFxAvKAkdtSUmjiYIjSJXHToK0i2XRhBQGBD85Y9aR3aOfU7t0559bvv7eZ79nF2rVlWtopr97lpVe1WqCkmSpnOf+W6AJGnhMywkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAuNrSR/n+SoWV7ncUk+MpvrnEtJViSpJNvOd1u0sBgWWtSSXJXkjiQ/Gnj91TDLVtWhVXXqqNs4rLYvv7vUt6nFyW8PWgqeUVVfnO9GSEuZZxZaspK8JMlXk/xVkluSfCvJQQPz/zHJy9v7hyf5p1bvxiQfH6j3m0kuaPMuSPKbA/P2bsvdlmQdsMukNuyf5J+T/DDJN5McuBX7cZ8ka5L8a5IfJDkzyc5t3kS30VFJvtfa/paBZbdLcmqSm5NckeSNSTa2eR8GlgN/287I3jiw2RdsaX0aX4aFlronAv9K9yF+LPDpiQ/aSd4BfAHYCdgD+EuAVvcc4ETgQcAJwDlJHtSW+xhwYVv/O4CfXQNJsqwt+z+AnYE3AJ9KsusM9+G/As8Engw8BLgZeO+kOk8CHgEcBPxJkke18mOBFcBDgacAL5xYoKpeBHyP7sxsh6r6syHWpzFlWGgp+Ez75j7xesXAvBuAd1fVT6rq48C3gcO2sI6fAHsBD6mq/19VX2nlhwFXVtWHq+ruqjod+BbwjCTLgccD/72q7qyqLwN/O7DOFwJrq2ptVf20qtYB64GnzXD/XgW8pao2VtWdwHHAsyddhH5bVd1RVd8Evgk8tpUfAfzPqrq5qjbShd4wplqfxpRhoaXgmVW148DrAwPzNtW/Hy3zarpv55O9EQhwfpLLkvyXVv6Qtsygq4Flbd7NVfXjSfMm7AU8ZzDI6L6x7z7D/dsLOGtgHVcA9wC7DdS5buD97cAOA+2/ZmDe4PvpTLU+jSnDQkvdsiQZmF4OfH9ypaq6rqpeUVUPAV4JvC/Jw1vdvSZVXw5sAq4Fdkqy/aR5E64BPjwpyLavquNnuA/XAIdOWs/9q2rTEMteS9etNmHPSfMddlpDMSy01D0YeE2S+yZ5DvAoYO3kSkmek2TiQ/Vmug/Rn7a6v5rk+Um2TfJcYB/g76rqarpupbcluV+SJwHPGFjtR+i6q56aZJsk909y4MB2tuS+rd7Ea1vgr4F3JtmrtXXXJIcPuf9nAm9OslO7hvLqSfOvp7ueIU3LsNBSMHE3z8TrrIF5XwNWAjcC7wSeXVU/2MI6Hg98LcmPgLOB11bVd1rdpwOvB35A11319Kq6sS33fLqL6DfRXUw+bWKFVXUNcDhwDLCZ7gzhj5n+/7u1wB0Dr+OA97Q2fSHJbcB5bZvDeDuwEfgu8EXgk8CdA/P/F/DW1sX1hiHXqTEUH36kpSrJS4CXV9WT5rstC0WSPwCOrKonz3dbtLh4ZiEtYUl2T3JA+63GI+jOkM7qW06azF9wS0vb/YD3A3sDPwTOAN43ry3SomQ3lCSpl91QkqReS7IbapdddqkVK1bMdzMkaVG58MILb6yqLQ5HsyTDYsWKFaxfv36+myFJi0qSyaMV/IzdUJKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqReI/sFd5I96R4EsxvdU8dOrqr3JDkOeAXdw2AAjqmqtW2ZNwMvo3u+8Guq6vOt/BC6B8BsA3xwKx5LOSMr1pwzytVP6arjD5uX7UpSn1EO93E38Pqq+nqSBwIXJlnX5r2rqv58sHKSfYAjgV+je8j8F5P8apv9XuApdE/8uiDJ2VV1+QjbLkkaMLKwqKpr6R4WT1XdluQKYNk0ixwOnFFVdwLfTbIBeEKbt6GqvgOQ5IxW17CQpDkyJ9cskqwAHkf3PGSAVye5OMkpSXZqZcvonlE8YWMrm6pckjRHRh4WSXYAPgW8rqpuBU4CHgbsS3fm8ReztJ3VSdYnWb958+b+BSRJQxtpWCS5L11QfLSqPg1QVddX1T1V9VPgA9zb1bQJ2HNg8T1a2VTl/05VnVxVq6pq1a67bnE4dknSVhpZWCQJ8CHgiqo6YaB894FqzwIube/PBo5M8ktJ9gZWAucDFwArk+yd5H50F8HPHlW7JUk/b5R3Qx0AvAi4JMlFrewY4HlJ9qW7nfYq4JUAVXVZkjPpLlzfDRxdVfcAJHk18Hm6W2dPqarLRthuSdIko7wb6itAtjBr7TTLvBN45xbK1063nCRptPwFtySpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXiMLiyR7JvlSksuTXJbkta185yTrklzZ/u7UypPkxCQbklycZL+BdR3V6l+Z5KhRtVmStGWjPLO4G3h9Ve0D7A8cnWQfYA1wblWtBM5t0wCHAivbazVwEnThAhwLPBF4AnDsRMBIkubGyMKiqq6tqq+397cBVwDLgMOBU1u1U4FntveHA6dV5zxgxyS7A08F1lXVTVV1M7AOOGRU7ZYk/bw5uWaRZAXwOOBrwG5VdW2bdR2wW3u/DLhmYLGNrWyq8snbWJ1kfZL1mzdvntX2S9K4G3lYJNkB+BTwuqq6dXBeVRVQs7Gdqjq5qlZV1apdd911NlYpSWpGGhZJ7ksXFB+tqk+34utb9xLt7w2tfBOw58Die7SyqcolSXNklHdDBfgQcEVVnTAw62xg4o6mo4DPDpS/uN0VtT9wS+uu+jxwcJKd2oXtg1uZJGmObDvCdR8AvAi4JMlFrewY4HjgzCQvA64Gjmjz1gJPAzYAtwMvBaiqm5K8A7ig1Xt7Vd00wnZLkiYZWVhU1VeATDH7oC3UL+DoKdZ1CnDK7LVOkjQT/oJbktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktRrqLBI8uhRN0SStHANe2bxviTnJ/nDJL8y0hZJkhacocKiqn4LeAGwJ3Bhko8lecpIWyZJWjCGvmZRVVcCbwXeBDwZODHJt5L8/qgaJ0laGIa9ZvGYJO8CrgB+B3hGVT2qvX/XCNsnSVoAth2y3l8CHwSOqao7Jgqr6vtJ3jqSlkmSFoxhw+Iw4I6qugcgyX2A+1fV7VX14ZG1TpK0IAx7zeKLwHYD0w9oZVNKckqSG5JcOlB2XJJNSS5qr6cNzHtzkg1Jvp3kqQPlh7SyDUnWDNleSdIsGjYs7l9VP5qYaO8f0LPM3wCHbKH8XVW1b3utBUiyD3Ak8Gttmfcl2SbJNsB7gUOBfYDntbqSpDk0bFj8OMl+ExNJfh24Y5r6VNWXgZuGXP/hwBlVdWdVfRfYADyhvTZU1Xeq6i7gjFZXkjSHhr1m8TrgE0m+DwT4D8Bzt3Kbr07yYmA98PqquhlYBpw3UGdjKwO4ZlL5E7e00iSrgdUAy5cv38qmSZK2ZNgf5V0APBL4A+BVwKOq6sKt2N5JwMOAfYFrgb/YinVM1caTq2pVVa3addddZ2u1kiSGP7MAeDywoi2zXxKq6rSZbKyqrp94n+QDwN+1yU10vw6fsEcrY5pySdIcGSosknyY7ozgIuCeVlzAjMIiye5VdW2bfBYwcafU2cDHkpwAPARYCZxP1+W1MsnedCFxJPD8mWxTkvSLG/bMYhWwT1XVsCtOcjpwILBLko3AscCBSfalC5qrgFcCVNVlSc4ELgfuBo4e+E3Hq4HPA9sAp1TVZcO2QZI0OzLM53+STwCvGTgrWNBWrVpV69ev3+rlV6w5ZxZbszhcdfxh890ESfMsyYVVtWpL84Y9s9gFuDzJ+cCdE4VV9Xuz0D5J0gI3bFgcN8pGSJIWtqHCoqr+KclewMqq+mKSB9BdQ5AkjYFhhyh/BfBJ4P2taBnwmVE1SpK0sAw73MfRwAHArfCzByE9eFSNkiQtLMOGxZ1tbCYAkmxLd/urJGkMDBsW/5TkGGC79uztTwB/O7pmSZIWkmHDYg2wGbiE7od0a+mexy1JGgPD3g31U+AD7SVJGjPDjg31XbZwjaKqHjrrLZIkLTgzGRtqwv2B5wA7z35zJEkL0bDPs/jBwGtTVb0bcDAhSRoTw3ZD7TcweR+6M42ZPAtDkrSIDfuBP/hEu7vphhc/YtZbI0lakIa9G+q3R90QSdLCNWw31B9NN7+qTpid5kiSFqKZ3A31eLrHnwI8g+6xp1eOolGSpIVl2LDYA9ivqm4DSHIccE5VvXBUDZMkLRzDDvexG3DXwPRdrUySNAaGPbM4DTg/yVlt+pnAqaNpkiRpoRn2bqh3Jvl74Lda0Uur6huja5YkaSEZthsK4AHArVX1HmBjkr1H1CZJ0gIz7GNVjwXeBLy5Fd0X+MioGiVJWliGPbN4FvB7wI8Bqur7wANH1ShJ0sIybFjcVVVFG6Y8yfaja5IkaaEZNizOTPJ+YMckrwC+iA9CkqSx0Xs3VJIAHwceCdwKPAL4k6paN+K2SZIWiN6wqKpKsraqHg0YEJI0hobthvp6ksePtCWSpAVr2F9wPxF4YZKr6O6ICt1Jx2NG1TBJ0sIxbVgkWV5V3wOeOkftkSQtQH3dUJ8BqKqrgROq6urB13QLJjklyQ1JLh0o2znJuiRXtr87tfIkOTHJhiQXDz7GNclRrf6VSY7a+l2VJG2tvrDIwPuHznDdfwMcMqlsDXBuVa0Ezm3TAIcCK9trNXASdOECHEvXDfYE4NiJgJEkzZ2+sKgp3veqqi8DN00qPpx7R6s9lW702ony06pzHt3vOXan6/5aV1U3VdXNdHdjTQ4gSdKI9V3gfmySW+nOMLZr7+HeC9y/PMPt7VZV17b313HvMzGWAdcM1NvYyqYq/zlJVtOdlbB8+fIZNkuSNJ1pw6KqthnVhtvvN2Z0ttKzvpOBkwFWrVo1a+uVJM1siPLZcH3rXqL9vaGVbwL2HKi3RyubqlySNIfmOizOBibuaDoK+OxA+YvbXVH7A7e07qrPAwcn2ald2D64lUmS5tCwP8qbsSSnAwcCuyTZSHdX0/F0gxK+DLgaOKJVXws8DdgA3A68FKCqbkryDuCCVu/tVTX5orkkacRGFhZV9bwpZh20hboFHD3Fek4BTpnFpkmSZmiuu6EkSYuQYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKnXtvPdAC0MK9acMy/bver4w+Zlu5JmxjMLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktRrXsIiyVVJLklyUZL1rWznJOuSXNn+7tTKk+TEJBuSXJxkv/losySNs/k8s/jtqtq3qla16TXAuVW1Eji3TQMcCqxsr9XASXPeUkkacwupG+pw4NT2/lTgmQPlp1XnPGDHJLvPRwMlaVzNV1gU8IUkFyZZ3cp2q6pr2/vrgN3a+2XANQPLbmxl/06S1UnWJ1m/efPmUbVbksbSfA338aSq2pTkwcC6JN8anFlVlaRmssKqOhk4GWDVqlUzWlaSNL15ObOoqk3t7w3AWcATgOsnupfa3xta9U3AngOL79HKJElzZM7DIsn2SR448R44GLgUOBs4qlU7Cvhse3828OJ2V9T+wC0D3VWSpDkwH91QuwFnJZnY/seq6nNJLgDOTPIy4GrgiFZ/LfA0YANwO/DSuW+yJI23OQ+LqvoO8NgtlP8AOGgL5QUcPQdNkyRNYSHdOitJWqAMC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9ZqPJ+VJP7NizTnzst2rjj9sXrYrLVaeWUiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF4O96GxNF/DjIBDjWhx8sxCktTLsJAk9bIbSppjjrSrxcgzC0lSr0UTFkkOSfLtJBuSrJnv9kjSOFkUYZFkG+C9wKHAPsDzkuwzv62SpPGxWK5ZPAHYUFXfAUhyBnA4cPm8tkpaRMbxduFx3OdRWSxhsQy4ZmB6I/DEwQpJVgOr2+SPkny7Z527ADfOWgsXl3Hdd/d7nuRP52Wz87rf87TP8Ivt915TzVgsYdGrqk4GTh62fpL1VbVqhE1asMZ1393v8eJ+z65Fcc0C2ATsOTC9RyuTJM2BxRIWFwArk+yd5H7AkcDZ89wmSRobi6IbqqruTvJq4PPANsApVXXZL7jaobuslqBx3Xf3e7y437MoVTWK9UqSlpDF0g0lSZpHhoUkqddYhsW4DB2SZM8kX0pyeZLLkry2le+cZF2SK9vfnea7raOQZJsk30jyd2167yRfa8f94+1miSUlyY5JPpnkW0muSPIb43C8k/y39m/80iSnJ7n/Uj3eSU5JckOSSwfKtniM0zmx/Te4OMl+W7vdsQuLMRs65G7g9VW1D7A/cHTb1zXAuVW1Eji3TS9FrwWuGJj+U+BdVfVw4GbgZfPSqtF6D/C5qnok8Fi6/V/SxzvJMuA1wKqq+o90N8EcydI93n8DHDKpbKpjfCiwsr1WAydt7UbHLiwYGDqkqu4CJoYOWXKq6tqq+np7fxvdB8cyuv09tVU7FXjm/LRwdJLsARwGfLBNB/gd4JOtypLb7yS/Avwn4EMAVXVXVf2QMTjedHd2bpdkW+ABwLUs0eNdVV8GbppUPNUxPhw4rTrnATsm2X1rtjuOYbGloUOWzVNb5kySFcDjgK8Bu1XVtW3WdcBu89SsUXo38Ebgp236QcAPq+ruNr0Uj/vewGbg/7Tutw8m2Z4lfryrahPw58D36ELiFuBClv7xHjTVMZ61z7txDIuxk2QH4FPA66rq1sF51d07vaTun07ydOCGqrpwvtsyx7YF9gNOqqrHAT9mUpfTEj3eO9F9g94beAiwPT/fTTM2RnWMxzEsxmrokCT3pQuKj1bVp1vx9ROnou3vDfPVvhE5APi9JFfRdTP+Dl1f/o6tmwKW5nHfCGysqq+16U/ShcdSP96/C3y3qjZX1U+AT9P9G1jqx3vQVMd41j7vxjEsxmbokNZP/yHgiqo6YWDW2cBR7f1RwGfnum2jVFVvrqo9qmoF3fH9h6p6AfAl4Nmt2lLc7+uAa5I8ohUdRDeM/5I+3nTdT/sneUD7Nz+x30v6eE8y1TE+G3hxuytqf+CWge6qGRnLX3AneRpdn/bE0CHvnOcmjUSSJwH/F7iEe/vuj6G7bnEmsBy4GjiiqiZfMFsSkhwIvKGqnp7koXRnGjsD3wBeWFV3zmf7ZluSfeku6t8P+A7wUrovhUv6eCd5G/BcujsAvwG8nK5vfskd7ySnAwfSDUV+PXAs8Bm2cIxbeP4VXbfc7cBLq2r9Vm13HMNCkjQz49gNJUmaIcNCktTLsJAk9TIsJEm9DAtJUi/DQkteknuSXDTwmnYgvSSvSvLiWdjuVUl2mUH9f0yy6hfd7qR17pjkDwemD5wYhVeaiUXxWFXpF3RHVe07bOWq+utRNmaO7Qj8IfC++W6IFjfPLDS22jf/P0tySZLzkzy8lR+X5A3t/Wva80AuTnJGK9s5yWda2XlJHtPKH5TkC+25Ch8EMrCtF7ZtXJTk/W2o/OnadnCSf0ny9SSfaON7TbT5ba38kiSPbOW7tucYXNYGELy6ndUcDzysbfd/t9XvkHufefHR9sMtaVqGhcbBdpO6oZ47MO+Wqno03a9c372FZdcAj6uqxwCvamVvA77Ryo4BTmvlxwJfqapfA86i+zUtSR5F9+viA9oZzj3AC6ZqbPuQfyvwu1W1H7Ae+KOBKje28pOANwxs+x/atj85se3W/n+tqn2r6o9b2eOA19E9z+WhdOMoSdOyG0rjYLpuqNMH/r5rC/MvBj6a5DN0QyoAPAn4zwBV9Q/tjOKX6Z4l8fut/JwkN7f6BwG/DlzQvsRvx/SD+e1P90H+1Vb/fsC/DMyfGBDywonttTY9q237cwPb3pLzq2ojQJKLgBXAV6apLxkWGns1xfsJh9GFwDOAtyR59FZsI8CpVfXmGdRfV1XPm2L+xPhG97B1/w8Pjo+0tevQmLEbSuPuuQN/B7+9k+Q+wJ5V9SXgTcCvADvQDc74glbnQLpuoVuBLwPPb+WHAhPPuj4XeHaSB7d5OyfZa5o2nQccMHANZfskv9qzH18Fjmj1Dx7Y9m3AA3uWlXr5jULjYLvW3TLhc1U1cfvsTkkupvu2Pfmb/DbAR9I9rjTAiVX1wyTHAae05W7n3qGh3wacnuQy4J/phs6mqi5P8lbgCy2AfgIcTTc66M+pqs1JXtLW9Uut+K3A/5tmHye2/SK60LsOuK2q7kzy1SSXAn8PnDPNOqQpOeqsxla6hyOtqqob57stv6gWKvdU1d1JfoPuaXlD3y4s9fHMQloalgNntjOXu4BXzHN7tMR4ZiFJ6uUFbklSL8NCktTLsJAk9TIsJEm9DAtJUq9/Ax5oz8aVXtXmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IttAkHl_rSRA",
        "colab_type": "code",
        "outputId": "39ae5e90-6476-4ada-9689-9aac9166203b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.hist(reward_list)\n",
        "plt.title('Episode Reward')\n",
        "plt.xlabel('Episode length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqqJhYO6n6Ei",
        "colab_type": "text"
      },
      "source": [
        "This is the tabular list of Q-values to determine the best action.\n",
        "The actions are ordered in the columns as left / down / right  / up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju0irrIQczAq",
        "colab_type": "code",
        "outputId": "535ccfec-004d-450d-83ca-352bf759c05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(q_table)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.13775986 0.10579267 0.11858893 0.11336296]\n",
            " [0.06201256 0.047707   0.04004209 0.08800725]\n",
            " [0.08605525 0.08758808 0.08580799 0.08736809]\n",
            " [0.04915629 0.04522387 0.01921714 0.08567113]\n",
            " [0.12641212 0.09052625 0.11866346 0.10571829]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08627082 0.06650097 0.06128341 0.01321445]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.18417972 0.18786766 0.05422604 0.21394355]\n",
            " [0.30593438 0.30561512 0.25453652 0.16295473]\n",
            " [0.29067408 0.19503914 0.10395681 0.22757036]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.17645462 0.11448183 0.44945988 0.44081817]\n",
            " [0.55328546 0.57222524 0.50874842 0.55768021]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zik5O1y_hlp",
        "colab_type": "text"
      },
      "source": [
        "Now let the agent run and see how we reach the goal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjtaqGDrGN7v",
        "colab_type": "code",
        "outputId": "cae7a171-cb61-4d66-d411-115dad9eb25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "state  = env.reset()\n",
        "\n",
        "\n",
        "for i in range(200):\n",
        "  action = np.argmax(q_table[state,:])\n",
        "  new_state, reward, done, info = env.step(action)\n",
        "  env.render()\n",
        "  print('Done? {}'.format(done))\n",
        "  state = new_state\n",
        "  if done:\n",
        "    print('Episode ends. Reward: {}, Number of steps: {}'.format(reward,i))\n",
        "    break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Done? False\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "Done? False\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "Done? False\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Done? False\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Done? True\n",
            "Episode ends. Reward: 1.0, Number of steps: 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIeLf91onNRG",
        "colab_type": "text"
      },
      "source": [
        "Show the same thing but as an animation at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkOrOuLIJsdy",
        "colab_type": "code",
        "outputId": "bba72378-d39d-407f-b16c-b57e30750f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "frames = []\n",
        "state  = env.reset()\n",
        "max_iter = 250\n",
        "\n",
        "for i in range(200):\n",
        "  action = np.argmax(q_table[state,:])\n",
        "  new_state, reward, done, info = env.step(action)\n",
        "  env.render()\n",
        "  print('Done? {}'.format(done))\n",
        "  state = new_state\n",
        "\n",
        "\n",
        "\n",
        "  frames.append({\n",
        "      'frame'  : env.render(mode='ansi'),\n",
        "      'state'  : state,\n",
        "      'action' : action,\n",
        "      'reward' : reward,\n",
        "      'done'   : done\n",
        "  })\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "print(len(frames))\n",
        "\n",
        "for i, frame in enumerate(frames):\n",
        "  clear_output(wait=True)\n",
        "  print(frame['frame'])\n",
        "  print(f\"Timestep: {i}\")\n",
        "  print(f\"State: {frame['state']}\")\n",
        "  print(f\"Action: {frame['action']}\")\n",
        "  print(f\"Reward: {frame['reward']}\")\n",
        "  print(f\"Done: {frame['done']}\")\n",
        "  time.sleep(.2)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Timestep: 39\n",
            "State: 15\n",
            "Action: 1\n",
            "Reward: 1.0\n",
            "Done: True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}